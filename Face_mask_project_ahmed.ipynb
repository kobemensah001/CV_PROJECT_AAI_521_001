{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "mount_file_id": "1SIzlUmWjkvJcz1fV8mi-iw4dKiWEO3V7",
      "authorship_tag": "ABX9TyMr0tsI4tYuShdCrqJf6OUl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kobemensah001/CV_PROJECT_AAI_521_001/blob/main/Face_mask_project_ahmed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNdsSKSTbO0X"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "annotations_path = '/content/drive/MyDrive/Face mask dataset/annotations'\n",
        "images_path = '/content/drive/MyDrive/Face mask dataset/images'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "metadata": {
        "id": "njGJbGbXSAx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import xml.etree.ElementTree as ET  # Import for parsing XML files\n",
        "\n",
        "annotations_path = '/content/drive/MyDrive/Face mask dataset/annotations'  # Your annotations path\n",
        "\n",
        "# Initialize the dataset dictionary\n",
        "dataset = {\n",
        "    \"file\":[],\n",
        "    \"name\":[],\n",
        "    \"width\":[],\n",
        "    \"height\":[],\n",
        "    \"xmin\":[],\n",
        "    \"ymin\":[],\n",
        "    \"xmax\":[],\n",
        "    \"ymax\":[],\n",
        "}\n",
        "\n",
        "# Iterate through each XML file in the annotations directory\n",
        "for anno in glob.glob(annotations_path + \"/*.xml\"):\n",
        "    tree = ET.parse(anno)\n",
        "    width, height = None, None  # Initialize width and height\n",
        "\n",
        "    for elem in tree.iter():\n",
        "        if 'size' in elem.tag:\n",
        "            for attr in list(elem):\n",
        "                if 'width' in attr.tag:\n",
        "                    width = int(round(float(attr.text)))\n",
        "                if 'height' in attr.tag:\n",
        "                    height = int(round(float(attr.text)))\n",
        "\n",
        "        if 'object' in elem.tag:\n",
        "            for attr in list(elem):\n",
        "                if 'name' in attr.tag:\n",
        "                    name = attr.text\n",
        "                    dataset['name'].append(name)\n",
        "                    dataset['width'].append(width)\n",
        "                    dataset['height'].append(height)\n",
        "                    dataset['file'].append(anno.split('/')[-1][:-4])\n",
        "\n",
        "                if 'bndbox' in attr.tag:\n",
        "                    for dim in list(attr):\n",
        "                        if 'xmin' in dim.tag:\n",
        "                            xmin = int(round(float(dim.text)))\n",
        "                            dataset['xmin'].append(xmin)\n",
        "                        if 'ymin' in dim.tag:\n",
        "                            ymin = int(round(float(dim.text)))\n",
        "                            dataset['ymin'].append(ymin)\n",
        "                        if 'xmax' in dim.tag:\n",
        "                            xmax = int(round(float(dim.text)))\n",
        "                            dataset['xmax'].append(xmax)\n",
        "                        if 'ymax' in dim.tag:\n",
        "                            ymax = int(round(float(dim.text)))\n",
        "                            dataset['ymax'].append(ymax)\n"
      ],
      "metadata": {
        "id": "qfXCWn54eYAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame(dataset)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Qm56VSwmehvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to parse an XML file and extract annotations\n",
        "def parse_xml(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "    annotations = []\n",
        "\n",
        "    for member in root.findall('object'):\n",
        "        label = member.find('name').text\n",
        "        bndbox = member.find('bndbox')\n",
        "        xmin = int(bndbox.find('xmin').text)\n",
        "        ymin = int(bndbox.find('ymin').text)\n",
        "        xmax = int(bndbox.find('xmax').text)\n",
        "        ymax = int(bndbox.find('ymax').text)\n",
        "        annotations.append({'label': label, 'bbox': [xmin, ymin, xmax, ymax]})\n",
        "\n",
        "    filename = root.find('filename').text\n",
        "    return filename, annotations"
      ],
      "metadata": {
        "id": "lbesR4ECT00q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess a single image and its annotations for a fixed size of 300x300\n",
        "def preprocess_image_annotation(image_path, annotation_list, target_size=(300, 300)):\n",
        "    # Load the image\n",
        "    image = Image.open(image_path)\n",
        "    image = image.convert('RGB')  # Ensure image is in RGB format\n",
        "\n",
        "    # Original size\n",
        "    original_size = image.size\n",
        "\n",
        "    # Resize image to 600x600\n",
        "    image = image.resize(target_size)\n",
        "\n",
        "    # Normalize the pixel values to be between 0 and 1\n",
        "    image_array = np.array(image) / 255.0\n",
        "\n",
        "    # Scale the bounding box annotations\n",
        "    scale_x = target_size[0] / original_size[0]\n",
        "    scale_y = target_size[1] / original_size[1]\n",
        "    scaled_annotations = []\n",
        "    for ann in annotation_list:\n",
        "        scaled_bbox = [int(ann['bbox'][0] * scale_x), int(ann['bbox'][1] * scale_y),\n",
        "                       int(ann['bbox'][2] * scale_x), int(ann['bbox'][3] * scale_y)]\n",
        "        scaled_annotations.append({'label': ann['label'], 'bbox': scaled_bbox})\n",
        "\n",
        "    return image_array, scaled_annotations"
      ],
      "metadata": {
        "id": "lJdPrnY0T7cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_dir = r'/content/drive/MyDrive/Face mask dataset/annotations'\n",
        "images_dir = r'/content/drive/MyDrive/Face mask dataset/images'\n"
      ],
      "metadata": {
        "id": "K2V1Dk7uUWLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all XML files in the annotations directory\n",
        "xml_files = [f for f in os.listdir(annotations_dir) if f.endswith('.xml')]"
      ],
      "metadata": {
        "id": "DBF7SdTvUGoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess all images and annotations\n",
        "preprocessed_data = []\n",
        "for xml_file in xml_files:\n",
        "    # Parse annotations\n",
        "    filename, annotations = parse_xml(os.path.join(annotations_dir, xml_file))\n",
        "\n",
        "    # Corresponding image path\n",
        "    image_filename = filename.replace('.xml', '.png')  # Ensure the extension matches your image files\n",
        "    image_path = os.path.join(images_dir, image_filename)\n",
        "\n",
        "    # Preprocess image and annotations\n",
        "    image_array, scaled_annotations = preprocess_image_annotation(image_path, annotations)\n",
        "\n",
        "    # Store in the list\n",
        "    preprocessed_data.append({'image': image_array, 'annotations': scaled_annotations})"
      ],
      "metadata": {
        "id": "zoRyUFSVVOcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize one example\n",
        "sample_image_data = preprocessed_data[0]['image']\n",
        "sample_annotations = preprocessed_data[0]['annotations']\n",
        "plt.imshow(sample_image_data)\n",
        "for ann in sample_annotations:\n",
        "    bbox = ann['bbox']\n",
        "    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],\n",
        "                             linewidth=1, edgecolor='r' if ann['label'] == 'without_mask' else 'g',\n",
        "                             facecolor='none')\n",
        "    plt.gca().add_patch(rect)\n",
        "    plt.text(bbox[0], bbox[1], ann['label'], color='yellow', verticalalignment='top',\n",
        "             bbox={'color': 'black', 'pad': 0})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TNPT2aDTVV_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\n",
        "\n",
        "# Convert annotations to a suitable format for model training\n",
        "labels = [1 if 'without_mask' in [ann['label'] for ann in data['annotations']] else 0 for data in preprocessed_data]\n",
        "\n",
        "# Prepare features and labels for the model\n",
        "features = np.array([data['image'] for data in preprocessed_data])\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Load a pre-trained VGG16 model as a base\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(300, 300, 3))\n",
        "base_model.trainable = False  # Freeze the base model layers\n",
        "\n",
        "# Create the model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Use 'softmax' for multiclass classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=16),\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Plotting training and validation loss and accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "epochs = range(1, 11)\n",
        "\n",
        "# Training and Validation Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
        "plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Training and Validation Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_accuracy}\")\n",
        "\n",
        "# Generate predictions and calculate additional metrics\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.round(y_pred).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
        "\n",
        "# Generate prediction probabilities for ROC curve\n",
        "y_pred_prob = model.predict(X_test).ravel()\n",
        "\n",
        "# Compute ROC curve and ROC area\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotting the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:0.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Metrics\n",
        "metrics = {\n",
        "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"ROC AUC\"],\n",
        "    \"Value\": [test_accuracy, precision, recall, f1, roc_auc]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(metrics_df)\n"
      ],
      "metadata": {
        "id": "OThQrq0LhYxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis:\n",
        "Training and Validation Accuracy:\n",
        "The training accuracy increased from 60.85% to 72.58% over 10 epochs, which is a good sign of learning.\n",
        "The validation accuracy also improved, reaching 75.44% in the final epoch.\n",
        "However, the gap between training and validation accuracy suggests some overfitting.\n",
        "Loss:\n",
        "Both training and validation loss decreased over epochs, indicating that the model is learning effectively.\n",
        "The fluctuations in validation loss suggest that the model might benefit from further tuning.\n",
        "Precision, Recall, and F1-Score:\n",
        "Precision is high (75%), indicating that when the model predicts a positive class, it is correct 75% of the time.\n",
        "Recall is low (28.85%), meaning the model is missing a significant number of actual positive cases.\n",
        "The F1-score, which balances precision and recall, is relatively low (41.67%). This indicates that the model is not effectively capturing the positive class.\n",
        "ROC AUC:\n",
        "The ROC AUC score is 76.57%, which is decent but indicates room for improvement, especially in distinguishing between the classes.\n",
        "Suggestions for Improvement:\n",
        "Address Overfitting:\n",
        "Consider adding more Dropout layers or increasing the dropout rate.\n",
        "Implement data augmentation if not already done.\n",
        "Experiment with regularization techniques like L1 or L2 regularization.\n",
        "Improve Recall:\n",
        "Since recall is low, consider adjusting the class weights to give more importance to the positive class.\n",
        "Experiment with different decision thresholds, instead of the default 0.5, to improve recall.\n",
        "Model Architecture and Hyperparameters:\n",
        "Try different model architectures or adjust the existing one.\n",
        "Experiment with different hyperparameters, like the number of neurons in Dense layers or learning rate.\n",
        "Cross-validation:\n",
        "Use k-fold cross-validation to ensure that the model's performance is consistent across different subsets of the training data.\n",
        "Additional Training:\n",
        "More training epochs might help, but be cautious of overfitting.\n",
        "Consider unfreezing some of the layers of the base model for fine-tuning.\n",
        "By implementing these suggestions, you should be able to improve the model's ability to generalize to new data, as well as its capacity to correctly identify more positive cases (thus improving recall and F1-score)."
      ],
      "metadata": {
        "id": "gU44976BitBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjusting the Learning Rate"
      ],
      "metadata": {
        "id": "PFpbbTU2aqZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Assuming 'model' is your Keras model and 'X_train', 'y_train' are your training data\n",
        "\n",
        "# Define the optimizer with a higher or lower learning rate\n",
        "optimizer = Adam(learning_rate=0.001)  # Adjust the learning rate as needed\n",
        "\n",
        "# Compile the model with the new optimizer\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Reduce learning rate when a metric has stopped improving\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[reduce_lr])\n",
        "\n",
        "# Now you can evaluate, make predictions, or plot the training history\n"
      ],
      "metadata": {
        "id": "VT1OdwqFaoRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the modification you've made to your model, it seems that you have altered the learning rate of the optimizer and added a callback to reduce the learning rate when the validation loss stops improving. These are common techniques to improve model performance, particularly in addressing issues like overfitting or helping the model to converge better.\n",
        "\n",
        "Given these modifications, here's how you can compare the performance of your model before and after these changes:\n",
        "\n",
        "Training and Evaluating the Original Model:\n",
        "Train your original model (without the learning rate adjustments and ReduceLROnPlateau callback).\n",
        "Evaluate the model on your test data and store the predictions.\n",
        "Optionally, save the trained model or its weights for future use.\n",
        "Applying Modifications and Retraining:\n",
        "Apply the modifications (as per your provided code).\n",
        "Retrain the model on the same training data.\n",
        "Evaluate the modified model on your test data and store these new predictions.\n",
        "Plot ROC Curves:\n",
        "Use the stored predictions from both model states to plot the ROC curves for comparison.\n",
        "Assuming you've done the above steps, here's how you can plot the ROC curves:\n",
        "\n",
        "python\n"
      ],
      "metadata": {
        "id": "-p4IGApceUPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " loss, accuracy, and the ROC curve. The ROC curve, in particular, provides a comprehensive view of the trade-off between the true positive rate and false positive rate across different thresholds, making it a useful tool for evaluating model performance.\n",
        "\n",
        "Performance Metrics Before Modification:\n",
        "Initial Training Loss: 0.6860\n",
        "Final Training Loss: 0.5248\n",
        "Initial Training Accuracy: 62.90%\n",
        "Final Training Accuracy: 72.87%\n",
        "Validation Loss and Accuracy showed fluctuations.\n",
        "Performance Metrics After Modification:\n",
        "Initial Training Loss: 0.6597\n",
        "Final Training Loss: 0.1633\n",
        "Initial Training Accuracy: 68.62%\n",
        "Final Training Accuracy: 92.11%\n",
        "Validation Loss started lower but increased slightly, suggesting potential overfitting.\n",
        "Analysis:\n",
        "Training Performance: The training loss decreased significantly, and the training accuracy increased notably after the modification. This indicates better learning and model fitting to the training data.\n",
        "Validation Performance: While the initial validation loss was lower, it did not decrease consistently, and there was a slight increase in later epochs. This might suggest overfitting, as the model is performing increasingly better on the training data but not as well on the validation data."
      ],
      "metadata": {
        "id": "xPfnxK5wev4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Modify the Model Architecture\n",
        "You can add Dropout layers to your existing model structure. Let's assume your model is a Sequential model with layers defined previously. The following is a template to add Dropout:"
      ],
      "metadata": {
        "id": "Ie7BrAc8fXKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c10H_rdkfegJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\n",
        "import pandas as pd\n",
        "\n",
        "# Example preprocessed_data loading (you should load your actual data here)\n",
        "# preprocessed_data = ...\n",
        "\n",
        "# Convert annotations to a suitable format for model training\n",
        "labels = [1 if 'without_mask' in [ann['label'] for ann in data['annotations']] else 0 for data in preprocessed_data]\n",
        "\n",
        "# Prepare features and labels for the model\n",
        "features = np.array([data['image'] for data in preprocessed_data])\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Load a pre-trained VGG16 model as a base\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(300, 300, 3))\n",
        "base_model.trainable = False  # Freeze the base model layers\n",
        "\n",
        "# Create the model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Use 'softmax' for multiclass classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(\n",
        "    train_datagen.flow(X_train, y_train, batch_size=16),\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Plotting training and validation loss and accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "epochs = range(1, 11)\n",
        "\n",
        "# Training and Validation Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
        "plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Training and Validation Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_accuracy}\")\n",
        "\n",
        "# Generate predictions and calculate additional metrics\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.round(y_pred).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
        "\n",
        "# Generate prediction probabilities for ROC curve\n",
        "y_pred_prob = model.predict(X_test).ravel()\n",
        "\n",
        "# Compute ROC curve and ROC area\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotting the ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:0.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Metrics Table\n",
        "metrics = {\n",
        "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"ROC AUC\"],\n",
        "    \"Value\": [test_accuracy, precision, recall, f1, roc_auc]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "metrics_df\n"
      ],
      "metadata": {
        "id": "DLc2nxuLkEK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Validation Accuracy:\n",
        "The training accuracy improved from 62.61% to 69.79% over 10 epochs.\n",
        "The validation accuracy reached 71.93% by the final epoch.\n",
        "The model shows reasonable learning, but there's a noticeable gap between training and validation accuracy, indicating some degree of overfitting.\n",
        "Loss:\n",
        "Both training and validation loss decreased, with some fluctuations in validation loss. This is typical in training and suggests that the model might benefit from further tuning or regularization.\n",
        "Precision, Recall, and F1-Score:\n",
        "Precision is moderate (53.13%), indicating that when the model predicts a positive class, it is correct about half of the time.\n",
        "Recall is higher (65.38%), meaning the model is better at identifying actual positive cases.\n",
        "The F1-score, which balances precision and recall, is 58.62%. This score is decent but could be improved.\n",
        "ROC AUC:\n",
        "The ROC AUC score is not provided for the VGG16 model."
      ],
      "metadata": {
        "id": "6uY93agtlD-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance Comparison Table:\n",
        "Metric\tVGG16 Model\tPrevious Model\n",
        "Accuracy\t71.93%\t75.44%\n",
        "Precision\t53.13%\t75.00%\n",
        "Recall\t65.38%\t28.85%\n",
        "F1-Score\t58.62%\t41.67%\n",
        "ROC AUC\tN/A\t76.57%\n",
        "Observations and Suggestions:"
      ],
      "metadata": {
        "id": "O8C-ExGGlRJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Performance Comparison Table:\n",
        "Metric\tVGG16 Model\tPrevious Model\n",
        "Accuracy\t71.93%\t75.44%\n",
        "Precision\t53.13%\t75.00%\n",
        "Recall\t65.38%\t28.85%\n",
        "F1-Score\t58.62%\t41.67%\n",
        "ROC AUC\tN/A\t76.57%\n"
      ],
      "metadata": {
        "id": "sAf9tNXClg5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations and Suggestions:\n",
        "Overall Performance: The VGG16 model has a higher recall but lower precision and accuracy compared to the previous model. This suggests that VGG16 is better at identifying positive cases but also makes more false positive errors.\n",
        "Overfitting: There is evidence of overfitting in the VGG16 model, as seen by the gap between training and validation accuracy.\n",
        "F1-Score: The F1-score is higher in the VGG16 model, indicating a better balance between precision and recall compared to the previous model.\n",
        "Suggestions for Improvement:\n",
        "Regularization: To address overfitting, consider adding more Dropout layers or increasing the dropout rate. Regularization techniques like L1 or L2 regularization can also be beneficial.\n",
        "Data Augmentation: More aggressive data augmentation can help the model generalize better.\n",
        "Model Tuning: Experiment with different architectures or hyperparameters, such as the number of neurons in Dense layers or learning rate adjustments.\n",
        "Fine-Tuning: Unfreezing some of the layers in the pre-trained base model and training them along with the top layers might improve performance.\n",
        "By adjusting these aspects, the VGG16 model's ability to generalize and its overall performance might be enhanced."
      ],
      "metadata": {
        "id": "g0qiehiZlarZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vg4oAaiZfoXT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}